# LLM Workshop Resources

A curated collection of readings, tutorials, and tools to support both the theory and practice of large language models. Organized by topic for quick navigation.
---

## 📓 Workshop Materials

- **Slides:** `IntroLLMs.pdf`
- **Notebook:** `class1.ipynb` — Colab exercises on tokenization, embeddings, SFT & RAG

---

## 📚 Foundations & Theory

- **[CS 597G: Large Language Models (Princeton, Fall ’22)](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/)**  
  Lecture notes and assignments covering the mathematics and algorithms behind LLMs.  
- **[CS 324: Modeling with LLMs (Stanford, Winter ’22)](https://stanford-cs324.github.io/winter2022/lectures/modeling/)**  
  Slides on probabilistic and transformer‑based language modeling.  
- **[Attention Is All You Need (Portuguese translation)](https://medium.com/@msmurilo/tradução-artigo-attention-is-all-you-need-2f7a4113b3be)**  
  The seminal transformer paper, translated and annotated in portuguese.
- **[Google Crash Course: LLMs](https://developers.google.com/machine-learning/crash-course/llm)**  
  Bite‑sized videos on training and deploying basic language models.   

---

## 🔧 Fine‑Tuning & Adaptation

- **[A Comprehensive Introduction to Fine‑Tuning LLMs](https://medium.com/@sahin.sa...a/a-comprehensive-introduction-to-fine-tuning-llms-4d1bcc95a83a)**  
  Step‑by‑step guide to full vs. parameter‑efficient fine‑tuning methods.  
- **[LLM Bootcamp (Full Stack Deep Learning)](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/#llm-foundations)**  
  Hands‑on tutorials on LoRA, adapters, and prefix‑tuning.
- **[Awesome LLM (Hannibal046)](https://github.com/Hannibal046/Awesome-LLM?tab=readme-ov-file)**  
  A GitHub repo aggregating tools for embedding, retrieval, and RAG pipelines.  

---

## 🎨 Visualization & Other Resources

- **[LLM Diagrams & Animations](https://bbycroft.net/llm)**  
  Interactive illustrations of transformer internals and token flow.
- **[Tiktokenizer](https://tiktokenizer.vercel.app/)**
  Iterative token simulator. 
- **[ULTRACHAŦ Map (Nomic Atlas)](https://atlas.nomic.ai/data/stingning/ultrachat-1/map)**  
  Visualization of post‑training conversation data and labeling workflows.
- [Hugging Face Inference playground](https://huggingface.co/spaces/huggingface/inference-playground)
  Web-based interface that allows users to interact with models hosted on the Hugging Face Hub without writing any code.
- **[Resources to Master LLMs (KDnuggets)](https://www.kdnuggets.com/a-comprehensive-list-of-resources-to-master-large-language-models)**  
  Aggregated tutorials, blog posts, and video playlists.
- **[FineWeb (pretraining dataset)](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)**
  Internet scan database used for pretraining.
- **[LLM arena for model ranking](https://lmarena.ai)**
  Comparisson of sota models.
- **[LLM studio for running models locally](https://lmstudio.ai)**
  Local AI toolkit for local models.

---

## 🎥 Video Lectures

- **[Andrej Karpathy: “Transformers from Scratch”](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=69s)**  
  Intuitive walkthrough of transformer architectures and self‑attention.  

---


## 🤝 Contributing

Feel free to suggest new resources or fixes via pull requests! Or get in contact with creator! 
