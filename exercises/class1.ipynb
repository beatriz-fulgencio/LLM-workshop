{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beatriz-fulgencio/LLM-workshop/blob/main/exercises/class1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05872907",
      "metadata": {
        "id": "05872907"
      },
      "source": [
        "#Part One: Environment Setup & Ollama Basics\n",
        "\n",
        "**Goals**\n",
        "- Prepare Google Colab (or local) environment\n",
        "- Install pinned versions of required libraries (LlamaIndex, embeddings, etc.)\n",
        "- Install and run **Ollama** locally on Colab\n",
        "- Sanityâ€‘check GPU/CPU and tokenization\n",
        "\n",
        "> âœ… You **only** need to run this once per runtime. Subsequent notebooks assume these deps are installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3fdef35",
      "metadata": {
        "id": "c3fdef35"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ”§ Runtime checks\n",
        "!nvidia-smi || echo \"No GPU found (that's fine, just slower)\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm"
      ],
      "metadata": {
        "id": "sUscV0yZNh_p"
      },
      "id": "sUscV0yZNh_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext colabxterm"
      ],
      "metadata": {
        "id": "LAzYoBhiXATP"
      },
      "id": "LAzYoBhiXATP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cb40d13e",
      "metadata": {
        "id": "cb40d13e"
      },
      "source": [
        "## Start a Colab terminal & install Ollama\n",
        "If you want to actually run models **locally** with Ollama in Colab, you can try the script below. Note that Colab VMs may kill long-running processes.\n",
        "\n",
        "This is just an example on how LLMs can be run on Command Line Interfaces\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lanch a CLI in colab and serve Ollama\n",
        "\n",
        "run the following commands in the cli\n",
        "\n",
        "\n",
        "```\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "ollama serve & u\n",
        "ollama pull orca-mini\n",
        "ollama pull mistral\n",
        "```\n",
        "\n",
        "After the models are downloaded run\n",
        "\n",
        "\n",
        "```\n",
        "ollama run orca-mini\n",
        "ollama run mistral\n",
        "```"
      ],
      "metadata": {
        "id": "cFGtvtrQafDc"
      },
      "id": "cFGtvtrQafDc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run this to launch CLI\n"
      ],
      "metadata": {
        "id": "VzhnYjBXN4GW"
      },
      "id": "VzhnYjBXN4GW"
    },
    {
      "cell_type": "code",
      "source": [
        "xterm"
      ],
      "metadata": {
        "id": "UcJCilZPXPm_"
      },
      "id": "UcJCilZPXPm_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question instructions\n",
        "1. Create a text field after each question to answer it\n",
        "2. You may use the provided material or other resources - even know LMMs (ChatGPT) - to complete these tasks\n",
        "  "
      ],
      "metadata": {
        "id": "62dGDtnMVSv6"
      },
      "id": "62dGDtnMVSv6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1\n",
        "  Give a brief description of how LLMs architecture works and specially how Ollama architecture works."
      ],
      "metadata": {
        "id": "9l6TFsGqTUfJ"
      },
      "id": "9l6TFsGqTUfJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "  What does the 1b and 7b as characteristics of the models?"
      ],
      "metadata": {
        "id": "4hf4aNITT5F1"
      },
      "id": "4hf4aNITT5F1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3\n",
        "\n",
        "Try running both models. In a note, write down their use of CPU, GPU RAM.\n",
        "Try asking `What are Large Language Models` and compare the answers. What are the main differences you encountered?\n",
        "Now, try asking ` What were the brazilian movies seen in cannes in 2025?` what does the model answer?"
      ],
      "metadata": {
        "id": "47TULdc2Xm6V"
      },
      "id": "47TULdc2Xm6V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4\n",
        "\n",
        "Now run\n",
        "\n",
        "```\n",
        "    ollama pull llama3.2-vision:11b\n",
        "```\n",
        "\n",
        "in the terminal and after it is pulled, run the model and feed it an image with the following command:\n",
        "\n",
        "\n",
        "```\n",
        "ollama run llama3.2-vision\n",
        "\n",
        "/[path_to_img] [question]\"\n",
        "```\n",
        "\n",
        "What process was done on the image so the models understands it?"
      ],
      "metadata": {
        "id": "xzottUCKY5iR"
      },
      "id": "xzottUCKY5iR"
    },
    {
      "cell_type": "markdown",
      "id": "9c23332e",
      "metadata": {
        "id": "9c23332e"
      },
      "source": [
        "# Part Two: Quick LLM Sanity Test and testing Llamaindex with simple RAG\n",
        "Below we will:\n",
        "1. Dwnload imports\n",
        "2. Configure LlamaIndex global `Settings`\n",
        "3. Run a simple prompt to ensure everything works\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title install pip dependencies\n",
        "!pip install llama-index llama-index-llms-ollama llama-index-embeddings-huggingface llama_index-readers-file langchain_community"
      ],
      "metadata": {
        "id": "bkevLQGEbirW"
      },
      "id": "bkevLQGEbirW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data\n",
        "1. Download a the simple HTML file from https://letterboxd.com/journal/best-of-cannes-2025/ and load it into this colab"
      ],
      "metadata": {
        "id": "fAtH5NsCcNDw"
      },
      "id": "fAtH5NsCcNDw"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "def loader(path):\n",
        "    documents = SimpleDirectoryReader(path, required_exts=['.html']).load_data()\n",
        "    return documents\n",
        "\n",
        "documents = loader(\"/content\")\n",
        "doc = documents[0]"
      ],
      "metadata": {
        "id": "SoreREeTcEOV"
      },
      "id": "SoreREeTcEOV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5:\n",
        "What other data loader could have been used that would have prevented us to download and store the html file in a directory first hand?"
      ],
      "metadata": {
        "id": "twERiTz6cbhi"
      },
      "id": "twERiTz6cbhi"
    },
    {
      "cell_type": "code",
      "source": [
        "#insert new code here"
      ],
      "metadata": {
        "id": "11BV2BDsVzWL"
      },
      "id": "11BV2BDsVzWL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6:\n",
        "Look at the generated document and its attributes. What can you say about the quality of the text ? Are all the information useful for future prompts answering ? What are the costs of having useless data in our document ? Can we give this directly to a LLM as an input ? What step is missing ?"
      ],
      "metadata": {
        "id": "mifTmHnIdM3g"
      },
      "id": "mifTmHnIdM3g"
    },
    {
      "cell_type": "code",
      "source": [
        "#insert new code here"
      ],
      "metadata": {
        "id": "0cBeSRWWWQOf"
      },
      "id": "0cBeSRWWWQOf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to chunk your text before embedding to make it easier to proccess"
      ],
      "metadata": {
        "id": "N0kiNwYgf0hw"
      },
      "id": "N0kiNwYgf0hw"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# Tune chunk sizes to balance context vs. retrieval precision\n",
        "splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
        "nodes = splitter.get_nodes_from_documents([doc]) #you can change doc to the name of your saved document\n",
        "\n"
      ],
      "metadata": {
        "id": "18_ZkP5eetmC"
      },
      "id": "18_ZkP5eetmC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Embedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "index = VectorStoreIndex(nodes)\n"
      ],
      "metadata": {
        "id": "DQRZsGR8euXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb3502a-15b6-41d3-e7a9-c3c5bd5d3b25"
      },
      "id": "DQRZsGR8euXo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
            "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7:\n",
        "What is the embedder used in our case ? What other embedder can be used and what are their differences ?"
      ],
      "metadata": {
        "id": "W4aGsv3be9N5"
      },
      "id": "W4aGsv3be9N5"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Query\n",
        "from llama_index.llms.ollama import Ollama\n",
        "Settings.llm = Ollama(model=\"mistral\", request_timeout=360.0)\n",
        "qe = index.as_query_engine()\n",
        "print(qe.query(\"What were the brazilian movies seen in cannes in 2025?\"))"
      ],
      "metadata": {
        "id": "Ju6er0EYgJ_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebfd797-bcfe-401f-f5de-e11f71ecc5a1"
      },
      "id": "Ju6er0EYgJ_T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In the provided context, two Brazilian films were showcased at Cannes in 2025. These are \"The Secret Agent\" directed by Kleber MendonÃ§a Filho and \"Sentimental Value\" directed by Joachim Trier.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 8:\n",
        "What does the model answer? Is it more accurate than before?"
      ],
      "metadata": {
        "id": "lHwwhDBPgWbw"
      },
      "id": "lHwwhDBPgWbw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 3: Workflow\n",
        "The goal was to create a system that can automatically recommend books on a specific topic and then further analyze those books by categorizing them into genres. This automation is achieved by leveraging the capabilities of a large language model (LLM).\n",
        "For this I defined a workflow structure, BookFlow consisting of two steps:\n",
        "`generate_book_list`: uses the LLM to generate a list of books about a given topic.\n",
        "`classify_books`: takes the generated book list and uses the LLM to classify the books into different genres.\n"
      ],
      "metadata": {
        "id": "akE3prDQgrCO"
      },
      "id": "akE3prDQgrCO"
    },
    {
      "cell_type": "code",
      "source": [
        "#add your code here"
      ],
      "metadata": {
        "id": "a-rcTTqJbVbu"
      },
      "id": "a-rcTTqJbVbu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}